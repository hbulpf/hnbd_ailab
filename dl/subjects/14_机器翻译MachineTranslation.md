机器翻译 Machine Translation 专知荟萃
# 入门学习
1. CIPS青工委学术专栏第9期 | 神经机器翻译 http://www.cipsc.org.cn/qngw/?p=953
1. 基于深度学习的机器翻译研究进展 http://www.caai.cn/index.php?s=/Home/Article/qikandetail/year/2016/month/02.html
1. 35张PPT带你深入浅出认识，深度学习的机器翻译 (也有视频教程)http://mp.weixin.qq.com/s/pnJDuXxw2VI9zEWgNivKdw
1. Kyunghyun Cho对神经机器翻译的介绍   [ https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/ ]   [ http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/ ]   [ https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/ ]
1. 神经网络机器翻译Neural Machine Translation(1): Encoder-Decoder Architecture (2): Attention Mechanism   [ http://blog.csdn.net/u011414416/article/details/51048994 ]   [ http://blog.csdn.net/u011414416/article/details/51057789 ]
1. TensorFlow 神经机器翻译教程   [ https://github.com/tensorflow/nmt ]
1. AMTA2016上Rico Sennrich的讲习班 http://statmt.org/mtma16/uploads/mtma16-neural.pdf

# 进阶论文
### 1997
1. Neco, R. P., & Forcada, M. L. (1997, June). Asynchronous translations with recurrent neural nets. In Neural Networks, 1997., International Conference on (Vol. 4, pp. 2535-2540). IEEE.  [ http://ieeexplore.ieee.org/document/614693/ ]
### 2003
1. Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.  [ http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf ]
1. Pascanu, R., Mikolov, T., & Bengio, Y. (2013, February). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning (pp. 1310-1318).  [ http://arxiv.org/abs/1211.5063 ]
### 2010
1. Sudoh, K., Duh, K., Tsukada, H., Hirao, T., & Nagata, M. (2010, July). Divide and translate: improving long distance reordering in statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (pp. 418-427). Association for Computational Linguistics.  [ https://dl.acm.org/citation.cfm?id=1868912 ]
### 2013
1. Kalchbrenner, N., & Blunsom, P. (2013, October). Recurrent Continuous Translation Models. In EMNLP (Vol. 3, No. 39, p. 413).  [ https://www.researchgate.net/publication/289758666_Recurrent_continuous_translation_models ]
### 2014
1. Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)  [ http://arxiv.org/abs/1406.6247 ]
1. Sutskever, I., Vinyals, O., & Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems(pp. 3104-3112).  [ https://arxiv.org/abs/1409.3215 ]
1. Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. . Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.  [ http://arxiv.org/abs/1406.1078 ]
1. Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.  [ https://arxiv.org/abs/1409.0473 ]
1. Jean, S., Cho, K., Memisevic, R., & Bengio, Y. (2014). On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007.  [ http://arxiv.org/abs/1412.2007 ]
1. Luong, M. T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. (2014). Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206.  [ http://arxiv.org/abs/1410.8206 ]
### 2015
1. Sennrich, R., Haddow, B., & Birch, A. (2015). Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709.  [ http://arxiv.org/abs/1511.06709 ]
1. Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015). Multi-Task Learning for Multiple Language Translation. In ACL (1) (pp. 1723-1732).  [ http://www.anthology.aclweb.org/P/P15/P15-1166.pdf ]
1. Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum risk training for neural machine translation. arXiv preprint arXiv:1512.02433.  [ https://arxiv.org/abs/1512.02433 ]
1. Bojar O, Chatterjee R, Federmann C, et al. Findings of the 2015 Workshop on Statistical Machine Translation  [ C ]. Tech Workshop on Statistical Machine Translation,2015.  [ https://www-test.pure.ed.ac.uk/portal/files/23139669/W15_3001.pdfv ]
### 2016
1. Facebook：Convolutional Sequence to Sequence Learning Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin  [ https://arxiv.org/abs/1705.03122 ]
1. Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … & Klingner, J. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.  [ https://arxiv.org/abs/1609.08144v1 ]
1. Gehring, J., Auli, M., Grangier, D., & Dauphin, Y. N. (2016). A convolutional encoder model for neural machine translation. arXiv preprint arXiv:1611.02344.  [ https://arxiv.org/abs/1611.02344 ]
1. Cheng, Y., Xu, W., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2016). Semi-supervised learning for neural machine translation. arXiv preprint arXiv:1606.04596.  [ http://arxiv.org/abs/1606.04596 ]
1. Wang, M., Lu, Z., Li, H., & Liu, Q. (2016). Memory-enhanced decoder for neural machine translation. arXiv preprint arXiv:1606.02003.  [ https://arxiv.org/abs/1606.02003 ]
1. Sennrich, R., & Haddow, B. (2016). Linguistic input features improve neural machine translation. arXiv preprint arXiv:1606.02892.  [ http://arxiv.org/abs/1606.02892 ]
1. Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling coverage for neural machine translation. arXiv preprint arXiv:1601.04811.  [ http://arxiv.org/abs/1601.04811 ]
1. Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C., & Haffari, G. (2016). Incorporating structural alignment biases into an attentional neural translation model. arXiv preprint arXiv:1601.01085.  [ http://www.m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1102.pdf ]
1. Hitschler, J., Schamoni, S., & Riezler, S. (2016). Multimodal pivots for image caption translation. arXiv preprint arXiv:1601.03916.  [ https://arxiv.org/abs/1601.03916 ]
1. Junczys-Dowmunt, M., Dwojak, T., & Hoang, H. (2016). Is neural machine translation ready for deployment. A case study on, 30.  [ https://arxiv.org/abs/1610.01108 ]
1. Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., … & Hughes, M. (2016). Google』s multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558.  [ https://arxiv.org/abs/1611.04558 ]
1. Bartolome, Diego, and Gema Ramirez.「Beyond the Hype of Neural Machine Translation,」MIT Technology Review (May 23, 2016), bit.ly/2aG4bvR.  [ https://www.slideshare.net/TAUS/beyond-the-hype-of-neural-machine-translation-diego-bartolome-tauyou-and-gema-ramirez-prompsit-language-engineering ]
1. Crego, J., Kim, J., Klein, G., Rebollo, A., Yang, K., Senellart, J., … & Enoue, S. (2016). SYSTRAN』s Pure Neural Machine Translation Systems. arXiv preprint arXiv:1610.05540.  [ https://arxiv.org/abs/1610.05540 ]
### 2017
1. Google：Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin  [ http://arxiv.org/abs/1706.03762 ]
1. Microsoft： Neural Phrase-based Machine Translation Po-Sen Huang, Chong Wang, Dengyong Zhou, Li Deng  [ http://arxiv.org/abs/1706.05565 ]
1. A Neural Network for Machine Translation, at Production Scale. (2017). Research Blog. Retrieved 26 July 2017, from   [ https://research.googleblog.com/2016/09/a-neural-network-for-machine.html ]  [ http://www.googblogs.com/a-neural-network-for-machine-translation-at-production-scale/ ]
1. Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017). Convolutional Sequence to Sequence Learning. arXiv preprint arXiv:1705.03122.  [ https://arxiv.org/abs/1705.03122 ]
1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762.  [ https://arxiv.org/abs/1706.03762 ]
1. Train Neural Machine Translation Models with Sockeye | Amazon Web Services. (2017). Amazon Web Services. Retrieved 26 July 2017, from  [ https://aws.amazon.com/blogs/ai/train-neural-machine-translation-models-with-sockeye/ ]
1. Dandekar, N. (2017). How does an attention mechanism work in deep learning for natural language processing?. Quora. Retrieved 26 July 2017, from  [ https://www.quora.com/How-does-an-attention-mechanism-work-in-deep-learning-for-natural-language-processing ]
1. Microsoft Translator launching Neural Network based translations for all its speech languages. (2017). Translator. Retrieved 27 July 2017, from  [ https://blogs.msdn.microsoft.com/translation/2016/11/15/microsoft-translator-launching-neural-network-based-translations-for-all-its-speech-languages/ ]
1. ACL 2017. (2017). Accepted Papers, Demonstrations and TACL Articles for ACL 2017.   [ online ] Available at:  [ https://chairs-blog.acl2017.org/2017/04/05/accepted-papers-and-demonstrations/ ]   [ Accessed 7 Aug. 2017 ].
### 综述
1. 神经机器翻译前沿进展 清华大学刘洋老师   [ http://crad.ict.ac.cn/CN/abstract/abstract3422.shtml ]
1. 斯坦福Thang Luong的博士论文   [ https://github.com/lmthang/thesis/blob/master/thesis.pdf ]
1. Deep Neural Networks in Machine Translation: An Overview   [ http://www.nlpr.ia.ac.cn/cip/ZongPublications/2015/IEEE-Zhang-8-5.pdf ]

# Tutorial
1. ACL 2016 Tutorial -- Neural Machine Translation Lmthang在ACL 2016上所做的tutorial   [ http://nlp.stanford.edu/projects/nmt/Luong-Cho-Manning-NMT-ACL2016-v4.pdf ]
1. 神经机器翻译前沿进展 由清华大学的刘洋老师在第十二届全国机器翻译讨论会（2016年8月在乌鲁木齐举办）上做的报告   [ http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt2016_ly_v3_160826.pptx ]
1. CCL2016 | T1B: 深度学习与机器翻译 第十五届全国计算语言学会议（CCL 2016）   [ http://www.cips-cl.org/static/CCL2016/tutorialsT1B.html ]
1. Neural Machine Translation   [ http://statmt.org/mtma16/uploads/mtma16-neural.pdf ]
1. ACL2016上Thang Luong，Kyunghyun Cho和Christopher Manning的讲习班   [ https://sites.google.com/site/acl16nmt/ ]
1. Kyunghyun Cho的talk : New Territory of Machine Translation，主要是讲cho自己所关注的NMT问题   [ https://drive.google.com/file/d/0B16RwCMQqrtdRVotWlQ3T2ZXTmM/view ]

# 视频教程
1. cs224d neural machine translation   [ https://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf ]   [ https://www.youtube.com/watch?v=IxQtK2SjWWM&index=11&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6 ]
1. 清华大学刘洋：基于深度学习的机器翻译
1. https://www.bilibili.com/video/av14782824/
1. PPT: http://mp.weixin.qq.com/s/pnJDuXxw2VI9zEWgNivKdw
1. A Practical Guide to Neural Machine Translation   [ https://www.youtube.com/watch?v=vxibD6VaOfI ]

# 代码
1. seq2seq 实现了谷歌提出的seq2seq模型，基于TensorFlow框架开发。   [ https://github.com/tensorflow/tensorflow ]
1. nmt.matlab 由Stanford的博士Lmthang开源的，代码由Matlab所写。  [ https://github.com/lmthang/nmt.matlab ]
1. GroundHog 实现了基于注意力机制的神经机器翻译模型，由Bengio研究组，基于Theano框架开发。   [ https://github.com/lisa-groundhog/GroundHog ]
1. NMT-Coverage 实现了基于覆盖率的神经机器翻译模型，由华为诺亚方舟实验室李航团队，基于Theano框架开发。   [ https://github.com/tuzhaopeng/NMT-Coverage ]
1. OpenNMT 由哈佛大学NLP组开源的神经机器翻译工具包，基于Torch框架开发，达到工业级程度。   [ http://opennmt.net/ ]
1. EUREKA-MangoNMT 由中科院自动化所的张家俊老师开发，采用C++。   [ https://github.com/jiajunzhangnlp/EUREKA-MangoNMT ]
1. dl4mt-tutorial 基于Theano框架开发。   [ https://github.com/nyu-dl/dl4mt-tutorial ]

# 领域专家
1. Université de Montréal： Yoshua Bengio，Dzmitry Bahdanau
1. New York University： KyungHyun Cho
1. Stanford University： Manning，Lmthang
1. Google： IIya Sutskever，Quoc V.Le
1. 中科院计算所： 刘群
1. 东北大学： 朱靖波
1. 清华大学： 刘洋
1. 中科院自动化所： 宗成庆，张家俊
1. 苏州大学： 熊德意，张民
1. 华为-诺亚方舟： 李航，涂兆鹏
1. 百度： 王海峰，吴华
