机器翻译 Machine Translation 专知荟萃
# 入门学习
# 进阶论文
综述
# Tutorial
# 视频教程
# 代码
# 领域专家


# 入门学习
CIPS青工委学术专栏第9期 | 神经机器翻译 http://www.cipsc.org.cn/qngw/?p=953
基于深度学习的机器翻译研究进展 http://www.caai.cn/index.php?s=/Home/Article/qikandetail/year/2016/month/02.html
35张PPT带你深入浅出认识，深度学习的机器翻译 (也有视频教程)http://mp.weixin.qq.com/s/pnJDuXxw2VI9zEWgNivKdw
Kyunghyun Cho对神经机器翻译的介绍 [https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/] [http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/] [https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/]
神经网络机器翻译Neural Machine Translation(1): Encoder-Decoder Architecture (2): Attention Mechanism [http://blog.csdn.net/u011414416/article/details/51048994] [http://blog.csdn.net/u011414416/article/details/51057789]
TensorFlow 神经机器翻译教程 [https://github.com/tensorflow/nmt]
AMTA2016上Rico Sennrich的讲习班 http://statmt.org/mtma16/uploads/mtma16-neural.pdf

# 进阶论文
1997
Neco, R. P., & Forcada, M. L. (1997, June). Asynchronous translations with recurrent neural nets. In Neural Networks, 1997., International Conference on (Vol. 4, pp. 2535-2540). IEEE.[http://ieeexplore.ieee.org/document/614693/]

2003
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.[http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf]
Pascanu, R., Mikolov, T., & Bengio, Y. (2013, February). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning (pp. 1310-1318).[http://arxiv.org/abs/1211.5063]

2010
Sudoh, K., Duh, K., Tsukada, H., Hirao, T., & Nagata, M. (2010, July). Divide and translate: improving long distance reordering in statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (pp. 418-427). Association for Computational Linguistics.[https://dl.acm.org/citation.cfm?id=1868912]

2013
Kalchbrenner, N., & Blunsom, P. (2013, October). Recurrent Continuous Translation Models. In EMNLP (Vol. 3, No. 39, p. 413).[https://www.researchgate.net/publication/289758666_Recurrent_continuous_translation_models]

2014
Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)[http://arxiv.org/abs/1406.6247]
Sutskever, I., Vinyals, O., & Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems(pp. 3104-3112).[https://arxiv.org/abs/1409.3215]
Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. . Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.[http://arxiv.org/abs/1406.1078]
Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.[https://arxiv.org/abs/1409.0473]
Jean, S., Cho, K., Memisevic, R., & Bengio, Y. (2014). On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007.[http://arxiv.org/abs/1412.2007]
Luong, M. T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. (2014). Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206.[http://arxiv.org/abs/1410.8206]

2015
Sennrich, R., Haddow, B., & Birch, A. (2015). Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709.[http://arxiv.org/abs/1511.06709]
Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015). Multi-Task Learning for Multiple Language Translation. In ACL (1) (pp. 1723-1732).[http://www.anthology.aclweb.org/P/P15/P15-1166.pdf]
Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum risk training for neural machine translation. arXiv preprint arXiv:1512.02433.[https://arxiv.org/abs/1512.02433]
Bojar O, Chatterjee R, Federmann C, et al. Findings of the 2015 Workshop on Statistical Machine Translation[C]. Tech Workshop on Statistical Machine Translation,2015.[https://www-test.pure.ed.ac.uk/portal/files/23139669/W15_3001.pdfv]

2016
Facebook：Convolutional Sequence to Sequence Learning Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin[https://arxiv.org/abs/1705.03122]
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … & Klingner, J. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.[https://arxiv.org/abs/1609.08144v1]
Gehring, J., Auli, M., Grangier, D., & Dauphin, Y. N. (2016). A convolutional encoder model for neural machine translation. arXiv preprint arXiv:1611.02344.[https://arxiv.org/abs/1611.02344]
Cheng, Y., Xu, W., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2016). Semi-supervised learning for neural machine translation. arXiv preprint arXiv:1606.04596.[http://arxiv.org/abs/1606.04596]
Wang, M., Lu, Z., Li, H., & Liu, Q. (2016). Memory-enhanced decoder for neural machine translation. arXiv preprint arXiv:1606.02003.[https://arxiv.org/abs/1606.02003]
Sennrich, R., & Haddow, B. (2016). Linguistic input features improve neural machine translation. arXiv preprint arXiv:1606.02892.[http://arxiv.org/abs/1606.02892]
Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling coverage for neural machine translation. arXiv preprint arXiv:1601.04811.[http://arxiv.org/abs/1601.04811]
Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C., & Haffari, G. (2016). Incorporating structural alignment biases into an attentional neural translation model. arXiv preprint arXiv:1601.01085.[http://www.m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1102.pdf]
Hitschler, J., Schamoni, S., & Riezler, S. (2016). Multimodal pivots for image caption translation. arXiv preprint arXiv:1601.03916.[https://arxiv.org/abs/1601.03916]
Junczys-Dowmunt, M., Dwojak, T., & Hoang, H. (2016). Is neural machine translation ready for deployment. A case study on, 30.[https://arxiv.org/abs/1610.01108]
Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., … & Hughes, M. (2016). Google』s multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558.[https://arxiv.org/abs/1611.04558]
Bartolome, Diego, and Gema Ramirez.「Beyond the Hype of Neural Machine Translation,」MIT Technology Review (May 23, 2016), bit.ly/2aG4bvR.[https://www.slideshare.net/TAUS/beyond-the-hype-of-neural-machine-translation-diego-bartolome-tauyou-and-gema-ramirez-prompsit-language-engineering]
Crego, J., Kim, J., Klein, G., Rebollo, A., Yang, K., Senellart, J., … & Enoue, S. (2016). SYSTRAN』s Pure Neural Machine Translation Systems. arXiv preprint arXiv:1610.05540.[https://arxiv.org/abs/1610.05540]

2017
Google：Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin[http://arxiv.org/abs/1706.03762]
Microsoft： Neural Phrase-based Machine Translation Po-Sen Huang, Chong Wang, Dengyong Zhou, Li Deng[http://arxiv.org/abs/1706.05565]
A Neural Network for Machine Translation, at Production Scale. (2017). Research Blog. Retrieved 26 July 2017, from [https://research.googleblog.com/2016/09/a-neural-network-for-machine.html][http://www.googblogs.com/a-neural-network-for-machine-translation-at-production-scale/]
Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017). Convolutional Sequence to Sequence Learning. arXiv preprint arXiv:1705.03122.[https://arxiv.org/abs/1705.03122]
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762.[https://arxiv.org/abs/1706.03762]
Train Neural Machine Translation Models with Sockeye | Amazon Web Services. (2017). Amazon Web Services. Retrieved 26 July 2017, from[https://aws.amazon.com/blogs/ai/train-neural-machine-translation-models-with-sockeye/]
Dandekar, N. (2017). How does an attention mechanism work in deep learning for natural language processing?. Quora. Retrieved 26 July 2017, from[https://www.quora.com/How-does-an-attention-mechanism-work-in-deep-learning-for-natural-language-processing]
Microsoft Translator launching Neural Network based translations for all its speech languages. (2017). Translator. Retrieved 27 July 2017, from[https://blogs.msdn.microsoft.com/translation/2016/11/15/microsoft-translator-launching-neural-network-based-translations-for-all-its-speech-languages/]
ACL 2017. (2017). Accepted Papers, Demonstrations and TACL Articles for ACL 2017. [online] Available at:[https://chairs-blog.acl2017.org/2017/04/05/accepted-papers-and-demonstrations/] [Accessed 7 Aug. 2017].

综述
神经机器翻译前沿进展 清华大学刘洋老师 [http://crad.ict.ac.cn/CN/abstract/abstract3422.shtml]
斯坦福Thang Luong的博士论文 [https://github.com/lmthang/thesis/blob/master/thesis.pdf]
Deep Neural Networks in Machine Translation: An Overview [http://www.nlpr.ia.ac.cn/cip/ZongPublications/2015/IEEE-Zhang-8-5.pdf]

# Tutorial
ACL 2016 Tutorial -- Neural Machine Translation Lmthang在ACL 2016上所做的tutorial [http://nlp.stanford.edu/projects/nmt/Luong-Cho-Manning-NMT-ACL2016-v4.pdf]
神经机器翻译前沿进展 由清华大学的刘洋老师在第十二届全国机器翻译讨论会（2016年8月在乌鲁木齐举办）上做的报告 [http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt2016_ly_v3_160826.pptx]
CCL2016 | T1B: 深度学习与机器翻译 第十五届全国计算语言学会议（CCL 2016） [http://www.cips-cl.org/static/CCL2016/tutorialsT1B.html]
Neural Machine Translation [http://statmt.org/mtma16/uploads/mtma16-neural.pdf]
ACL2016上Thang Luong，Kyunghyun Cho和Christopher Manning的讲习班 [https://sites.google.com/site/acl16nmt/]
Kyunghyun Cho的talk : New Territory of Machine Translation，主要是讲cho自己所关注的NMT问题 [https://drive.google.com/file/d/0B16RwCMQqrtdRVotWlQ3T2ZXTmM/view]

# 视频教程
cs224d neural machine translation [https://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf] [https://www.youtube.com/watch?v=IxQtK2SjWWM&index=11&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6]
清华大学刘洋：基于深度学习的机器翻译
https://www.bilibili.com/video/av14782824/
PPT: http://mp.weixin.qq.com/s/pnJDuXxw2VI9zEWgNivKdw
A Practical Guide to Neural Machine Translation [https://www.youtube.com/watch?v=vxibD6VaOfI]

# 代码
seq2seq 实现了谷歌提出的seq2seq模型，基于TensorFlow框架开发。 [https://github.com/tensorflow/tensorflow]
nmt.matlab 由Stanford的博士Lmthang开源的，代码由Matlab所写。[https://github.com/lmthang/nmt.matlab]
GroundHog 实现了基于注意力机制的神经机器翻译模型，由Bengio研究组，基于Theano框架开发。 [https://github.com/lisa-groundhog/GroundHog]
NMT-Coverage 实现了基于覆盖率的神经机器翻译模型，由华为诺亚方舟实验室李航团队，基于Theano框架开发。 [https://github.com/tuzhaopeng/NMT-Coverage]
OpenNMT 由哈佛大学NLP组开源的神经机器翻译工具包，基于Torch框架开发，达到工业级程度。 [http://opennmt.net/]
EUREKA-MangoNMT 由中科院自动化所的张家俊老师开发，采用C++。 [https://github.com/jiajunzhangnlp/EUREKA-MangoNMT]
dl4mt-tutorial 基于Theano框架开发。 [https://github.com/nyu-dl/dl4mt-tutorial]

# 领域专家
Université de Montréal： Yoshua Bengio，Dzmitry Bahdanau
New York University： KyungHyun Cho
Stanford University： Manning，Lmthang
Google： IIya Sutskever，Quoc V.Le
中科院计算所： 刘群
东北大学： 朱靖波
清华大学： 刘洋
中科院自动化所： 宗成庆，张家俊
苏州大学： 熊德意，张民
华为-诺亚方舟： 李航，涂兆鹏
百度： 王海峰，吴华
